# Visual Diagram Templates for Medium Blog ğŸ¨

## 1. **Attention Mechanism Flow**

```
Input Sentence: "The cat sat on the mat"

Step 1: Word Embeddings
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ The â”‚ â”‚ cat â”‚ â”‚ sat â”‚ â”‚ on  â”‚ â”‚ the â”‚ â”‚ mat â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
   â†“       â†“       â†“       â†“       â†“       â†“
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ 0.2 â”‚ â”‚ 0.8 â”‚ â”‚ 0.3 â”‚ â”‚ 0.1 â”‚ â”‚ 0.2 â”‚ â”‚ 0.4 â”‚
â”‚ 0.5 â”‚ â”‚ 0.1 â”‚ â”‚ 0.9 â”‚ â”‚ 0.6 â”‚ â”‚ 0.5 â”‚ â”‚ 0.7 â”‚
â”‚ 0.1 â”‚ â”‚ 0.6 â”‚ â”‚ 0.2 â”‚ â”‚ 0.3 â”‚ â”‚ 0.1 â”‚ â”‚ 0.3 â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜

Step 2: Attention Calculation (Focus on "sat")
     Query: "sat" = [0.3, 0.9, 0.2]
     
     Similarity with each word:
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Word  â”‚ Similarity â”‚ Attention (%) â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ The   â”‚    0.63    â”‚     28%       â”‚
     â”‚ cat   â”‚    0.85    â”‚     42%       â”‚ â† Most important!
     â”‚ sat   â”‚    1.00    â”‚     15%       â”‚
     â”‚ on    â”‚    0.32    â”‚      8%       â”‚
     â”‚ the   â”‚    0.25    â”‚      5%       â”‚
     â”‚ mat   â”‚    0.18    â”‚      2%       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: Weighted Output
Final "sat" representation = 
   0.28 Ã— The + 0.42 Ã— cat + 0.15 Ã— sat + 0.08 Ã— on + 0.05 Ã— the + 0.02 Ã— mat
```

## 2. **Transformer Architecture (Simplified)**

```
                    TRANSFORMER ENCODER
                    
Input: "The cat sat"
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Word Embeddings                â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”              â”‚
    â”‚    â”‚ The â”‚ â”‚ cat â”‚ â”‚ sat â”‚              â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Positional Encoding              â”‚
    â”‚         + + +                           â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”              â”‚
    â”‚    â”‚Pos 0â”‚ â”‚Pos 1â”‚ â”‚Pos 2â”‚              â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           ENCODER LAYER 1               â•‘
    â•‘                                         â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
    â•‘  â”‚      Multi-Head Attention          â”‚ â•‘
    â•‘  â”‚                                   â”‚ â•‘
    â•‘  â”‚  Head1  Head2  Head3  ... Head8   â”‚ â•‘
    â•‘  â”‚    â†˜      â†˜      â†˜        â†˜      â”‚ â•‘
    â•‘  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â•‘
    â•‘  â”‚      â”‚    Concatenate      â”‚      â”‚ â•‘
    â•‘  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
    â•‘              â†“                          â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
    â•‘  â”‚         Add & Norm                 â”‚ â•‘
    â•‘  â”‚    Input + Attention Output        â”‚ â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
    â•‘              â†“                          â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
    â•‘  â”‚      Feed Forward Network          â”‚ â•‘
    â•‘  â”‚                                   â”‚ â•‘
    â•‘  â”‚   Linear â†’ ReLU â†’ Linear          â”‚ â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
    â•‘              â†“                          â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
    â•‘  â”‚         Add & Norm                 â”‚ â•‘
    â•‘  â”‚      Input + FFN Output            â”‚ â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         â†“
    ... (5 more similar layers)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              OUTPUT                     â”‚
    â”‚         Rich Representations           â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”              â”‚
    â”‚    â”‚ The â”‚ â”‚ cat â”‚ â”‚ sat â”‚              â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. **Before vs After Transformers**

```
BEFORE TRANSFORMERS (RNNs)
Sequential Processing - Slow & Limited Memory

"The cat sat on the mat"

Step 1: Process "The"
â”Œâ”€â”€â”€â”€â”€â”
â”‚ The â”‚ â†’ Hidden State 1
â””â”€â”€â”€â”€â”€â”˜

Step 2: Process "cat" 
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ The â”‚ â”‚ cat â”‚ â†’ Hidden State 2 (forgets some of "The")
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜

Step 3: Process "sat"
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ The â”‚ â”‚ cat â”‚ â”‚ sat â”‚ â†’ Hidden State 3 (forgets more)
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜

... by the end, "The" is mostly forgotten!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AFTER TRANSFORMERS
Parallel Processing - Fast & Full Memory

"The cat sat on the mat"

All words processed simultaneously:
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ The â”‚ â”‚ cat â”‚ â”‚ sat â”‚ â”‚ on  â”‚ â”‚ the â”‚ â”‚ mat â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
   â†•       â†•       â†•       â†•       â†•       â†•
   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
           â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
                   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
                           â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
                                   â””â”€â”€â”€â”€â”€â”€â”€â”˜

Every word can attend to every other word!
All information preserved and accessible.
```

## 4. **Multi-Head Attention Specialization**

```
INPUT: "The programmer debugged the complex algorithm"

HEAD 1 - SYNTACTIC RELATIONSHIPS (Grammar)
Strong connections:
The â†â†’ programmer (determiner-noun)
programmer â†â†’ debugged (subject-verb)  
the â†â†’ algorithm (determiner-noun)

HEAD 2 - SEMANTIC RELATIONSHIPS (Meaning)
Strong connections:
programmer â†â†’ debugged (who does what)
debugged â†â†’ algorithm (what is debugged)
complex â†â†’ algorithm (what is complex)

HEAD 3 - POSITIONAL RELATIONSHIPS (Distance)
Strong connections between adjacent words:
The â†â†’ programmer
programmer â†â†’ debugged
debugged â†â†’ the
the â†â†’ complex
complex â†â†’ algorithm

HEAD 4 - REFERENCE RELATIONSHIPS (Pronouns/References)
In this case, no pronouns, but would connect:
- Pronouns to their referents
- Coreference resolution
```

## 5. **Attention Heatmap Visualization**

```
ATTENTION WEIGHTS MATRIX
(Darker = Higher Attention)

Sentence: "The cat sat on the mat"

           T  c  s  o  t  m
           h  a  a  n  h  a
           e  t  t     e  t

The     [â– â– â– â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡]  â† "The" pays most attention to itself
        
cat     [â–¡â–¡â– â– â– â– â– â– â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡]  â† "cat" focuses on itself and "sat"
        
sat     [â–¡â–¡â– â– â– â– â– â– â– â– â– â– â–¡â–¡â–¡â–¡â–¡â–¡]  â† "sat" strongly attends to "cat"
        
on      [â–¡â–¡â–¡â–¡â–¡â–¡â– â– â– â– â– â– â– â– â–¡â–¡â–¡â–¡]  â† "on" connects to "sat" and "mat"
        
the     [â– â– â– â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â– â– â– â– â– â– ]  â† Second "the" connects to first "The"
        
mat     [â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â– â– â– â– â– â– â– â– ]  â† "mat" focuses on "on" and itself

Legend:
â– â– â– â–  = High attention (0.7-1.0)
â– â– â–   = Medium attention (0.4-0.7)  
â– â–    = Low attention (0.1-0.4)
â–¡    = Minimal attention (0.0-0.1)
```

## 6. **Mathematical Intuition Visual**

```
ATTENTION CALCULATION STEP-BY-STEP

Given: Query="sat", Keys=["The","cat","sat","on","the","mat"]

Step 1: Dot Product (Similarity)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query: [0.3, 0.9, 0.2]                                 â”‚
â”‚                                                         â”‚
â”‚ Keys:                      Dot Products:                â”‚
â”‚ "The": [0.1, 0.3, 0.4] â†’  0.3Ã—0.1 + 0.9Ã—0.3 + 0.2Ã—0.4 â”‚
â”‚                         â†’  0.03 + 0.27 + 0.08 = 0.38   â”‚
â”‚                                                         â”‚
â”‚ "cat": [0.8, 0.1, 0.6] â†’  0.3Ã—0.8 + 0.9Ã—0.1 + 0.2Ã—0.6 â”‚
â”‚                         â†’  0.24 + 0.09 + 0.12 = 0.45   â”‚
â”‚                                                         â”‚
â”‚ "sat": [0.3, 0.9, 0.2] â†’  0.3Ã—0.3 + 0.9Ã—0.9 + 0.2Ã—0.2 â”‚
â”‚                         â†’  0.09 + 0.81 + 0.04 = 0.94   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Scale by âˆšd_k
d_k = 3, so âˆšd_k = 1.73
Scaled scores: [0.38/1.73, 0.45/1.73, 0.94/1.73]
              = [0.22, 0.26, 0.54]

Step 3: Softmax (Convert to Probabilities)
exp(0.22) = 1.25, exp(0.26) = 1.30, exp(0.54) = 1.72
Sum = 1.25 + 1.30 + 1.72 = 4.27

Probabilities: [1.25/4.27, 1.30/4.27, 1.72/4.27]
              = [0.29, 0.30, 0.40]
              = [29%, 30%, 40%]

Step 4: Weighted Sum with Values
Final output = 0.29Ã—Value("The") + 0.30Ã—Value("cat") + 0.40Ã—Value("sat")
```
